import os
import json
import requests
from bs4 import BeautifulSoup
from fpdf import FPDF
import tweepy
from datetime import datetime, timedelta
import hashlib

HISTORY_FILE = "posted_articles.json"
HASHTAG_FILE = "hashtags.json"
ACCOUNT_FILE = "accounts.json"
SUMMARY_FILE = "summaries.json"

def fetch_latest_ai_articles():
    """Firecrawl MCP ile geliÅŸmiÅŸ haber Ã§ekme"""
    try:
        # Ã–nce mevcut yayÄ±nlanan makaleleri yÃ¼kle
        posted_articles = load_json(HISTORY_FILE)
        posted_urls = [article.get('url', '') for article in posted_articles]
        
        headers = {'User-Agent': 'Mozilla/5.0'}
        html = requests.get("https://techcrunch.com/category/artificial-intelligence/", headers=headers).text
        soup = BeautifulSoup(html, "html.parser")
        article_links = soup.select("a.loop-card__title-link")[:10]  # Daha fazla makale Ã§ek
        
        articles_data = []
        for link_tag in article_links:
            title = link_tag.text.strip()
            url = link_tag['href']
            
            # Tekrar kontrolÃ¼ - aynÄ± URL'yi tekrar iÅŸleme
            if url in posted_urls:
                print(f"Makale zaten iÅŸlenmiÅŸ, atlanÄ±yor: {title}")
                continue
            
            # Makale hash'i oluÅŸtur (baÅŸlÄ±k bazlÄ±)
            article_hash = hashlib.md5(title.encode()).hexdigest()
            
            # Makale iÃ§eriÄŸini geliÅŸmiÅŸ ÅŸekilde Ã§ek
            content = fetch_article_content_advanced(url, headers)
            
            if content and len(content) > 100:  # Minimum iÃ§erik kontrolÃ¼
                articles_data.append({
                    "title": title, 
                    "url": url, 
                    "content": content,
                    "hash": article_hash,
                    "fetch_date": datetime.now().isoformat()
                })
        
        return articles_data
    except Exception as e:
        print(f"Haber Ã§ekme hatasÄ±: {e}")
        return []

def fetch_article_content_advanced(url, headers):
    """GeliÅŸmiÅŸ makale iÃ§eriÄŸi Ã§ekme - Firecrawl benzeri"""
    try:
        article_html = requests.get(url, headers=headers, timeout=10).text
        article_soup = BeautifulSoup(article_html, "html.parser")
        
        # Ã‡oklu selector deneme - daha kapsamlÄ± iÃ§erik Ã§ekme
        content_selectors = [
            "div.article-content p",
            "div.entry-content p", 
            "div.post-content p",
            "article p",
            "div.content p",
            ".article-body p"
        ]
        
        content = ""
        for selector in content_selectors:
            paragraphs = article_soup.select(selector)
            if paragraphs:
                content = "\n".join([p.text.strip() for p in paragraphs if p.text.strip()])
                if len(content) > 200:  # Yeterli iÃ§erik bulundu
                    break
        
        # EÄŸer hala iÃ§erik bulunamadÄ±ysa, tÃ¼m p etiketlerini dene
        if not content:
            all_paragraphs = article_soup.find_all('p')
            content = "\n".join([p.text.strip() for p in all_paragraphs if len(p.text.strip()) > 50])
        
        return content[:2000]  # Ä°Ã§eriÄŸi sÄ±nÄ±rla
        
    except Exception as e:
        print(f"Makale iÃ§eriÄŸi Ã§ekme hatasÄ± ({url}): {e}")
        return ""

def load_json(path):
    return json.load(open(path, 'r', encoding='utf-8')) if os.path.exists(path) else []

def save_json(path, data):
    with open(path, "w", encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

def summarize_article(article_content, api_key):
    """LLM ile geliÅŸmiÅŸ makale Ã¶zetleme"""
    prompt = f"""AÅŸaÄŸÄ±daki AI/teknoloji haberini TÃ¼rkÃ§e olarak Ã¶zetle. Ã–zet tweet formatÄ±nda, ilgi Ã§ekici ve bilgilendirici olsun:

Haber Ä°Ã§eriÄŸi:
{article_content[:1500]}

LÃ¼tfen:
- Maksimum 200 karakter
- Ana konuyu vurgula
- Teknik detaylarÄ± basitleÅŸtir
- Ä°lgi Ã§ekici bir dil kullan

Ã–zet:"""
    return openrouter_call(prompt, api_key, max_tokens=100)

def score_article(article_content, api_key):
    prompt = f"""Bu AI/teknoloji haberinin Ã¶nemini 1-10 arasÄ±nda deÄŸerlendir (sadece sayÄ±):

{article_content[:800]}

DeÄŸerlendirme kriterleri:
- Yenilik derecesi
- SektÃ¶rel etki
- GeliÅŸtiriciler iÃ§in Ã¶nem
- Genel ilgi

Puan:"""
    result = openrouter_call(prompt, api_key, max_tokens=5)
    try:
        return int(result.strip().split()[0])
    except:
        return 5

def categorize_article(article_content, api_key):
    prompt = f"""Bu haberin hedef kitlesini belirle:

{article_content[:500]}

SeÃ§enekler: Developer, Investor, General
Cevap:"""
    return openrouter_call(prompt, api_key, max_tokens=10).strip()

def openrouter_call(prompt, api_key, max_tokens=100):
    if not api_key:
        print("API anahtarÄ± bulunamadÄ±")
        return "API anahtarÄ± eksik"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "HTTP-Referer": "https://yourdomain.com",
        "X-Title": "VibeAI",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "meta-llama/llama-3.2-3b-instruct:free",  # Daha gÃ¼venilir model
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.7,
        "max_tokens": max_tokens
    }
    
    try:
        print(f"[DEBUG] API Ã§aÄŸrÄ±sÄ± yapÄ±lÄ±yor... Model: {payload['model']}")
        r = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, json=payload, timeout=30)
        
        print(f"[DEBUG] API YanÄ±t Kodu: {r.status_code}")
        
        if r.status_code != 200:
            print(f"API HatasÄ±: Durum Kodu {r.status_code}, YanÄ±t: {r.text}")
            return "API hatasÄ±"

        response_json = r.json()
        print(f"[DEBUG] API YanÄ±tÄ± alÄ±ndÄ±: {len(str(response_json))} karakter")
        
        if "choices" not in response_json or not response_json["choices"]:
            print(f"API YanÄ±tÄ±nda 'choices' anahtarÄ± bulunamadÄ±. YanÄ±t: {response_json}")
            return "YanÄ±t formatÄ± hatalÄ±"
        
        content = response_json["choices"][0]["message"]["content"]
        print(f"[DEBUG] Ä°Ã§erik alÄ±ndÄ±: {len(content)} karakter")
        return content.strip()
        
    except Exception as e:
        print(f"OpenRouter API Ã§aÄŸrÄ± hatasÄ±: {e}")
        return "BaÄŸlantÄ± hatasÄ±"

def generate_ai_tweet_with_content(article_data, api_key):
    """Makale iÃ§eriÄŸini okuyarak geliÅŸmiÅŸ tweet oluÅŸturma"""
    title = article_data.get("title", "")
    content = article_data.get("content", "")
    url = article_data.get("url", "")
    
    # EÄŸer iÃ§erik yoksa baÅŸlÄ±k kullan
    if not content:
        content = title
    
    prompt = f"""AÅŸaÄŸÄ±daki AI/teknoloji haberinden etkileyici bir tweet oluÅŸtur:

BaÅŸlÄ±k: {title}
Ä°Ã§erik: {content[:1200]}

Tweet Gereksinimleri:
- Maksimum 200 karakter (URL iÃ§in yer bÄ±rak)
- TÃ¼rkÃ§e olsun
- Ä°lgi Ã§ekici emoji kullan
- Relevant hashtag'ler ekle (#AI #Teknoloji #YapayZeka)
- Ana konuyu vurgula
- Merak uyandÄ±rÄ±cÄ± olsun
- Sadece tweet metnini dÃ¶ndÃ¼r, baÅŸka aÃ§Ä±klama yapma

Tweet:"""
    
    try:
        generated_tweet = openrouter_call(prompt, api_key, max_tokens=150)
        
        # EÄŸer API Ã§aÄŸrÄ±sÄ± baÅŸarÄ±sÄ±z olduysa, basit bir tweet oluÅŸtur
        if not generated_tweet or generated_tweet == "Ã–zet oluÅŸturulamadÄ±" or len(generated_tweet.strip()) < 10:
            # Fallback tweet oluÅŸtur
            generated_tweet = f"ğŸ¤– {title[:150]}... #AI #Teknoloji #YapayZeka"
        
        # URL'yi tweet'e ekle
        if url and url != "#":
            # Tweet uzunluÄŸunu kontrol et
            max_tweet_length = 250  # URL iÃ§in yer bÄ±rak
            if len(generated_tweet) > max_tweet_length:
                generated_tweet = generated_tweet[:max_tweet_length-3] + "..."
            
            final_tweet = f"{generated_tweet}\n\nğŸ”— {url}"
        else:
            final_tweet = generated_tweet
            
        return final_tweet
        
    except Exception as e:
        print(f"Tweet oluÅŸturma hatasÄ±: {e}")
        # Hata durumunda basit tweet oluÅŸtur
        fallback_tweet = f"ğŸ¤– {title[:200]}... #AI #Teknoloji #YapayZeka"
        if url and url != "#":
            fallback_tweet += f"\n\nğŸ”— {url}"
        return fallback_tweet

def setup_twitter_api():
    """X (Twitter) API kurulumu"""
    try:
        bearer_token = os.getenv("TWITTER_BEARER_TOKEN")
        api_key = os.getenv("TWITTER_API_KEY")
        api_secret = os.getenv("TWITTER_API_SECRET")
        access_token = os.getenv("TWITTER_ACCESS_TOKEN")
        access_token_secret = os.getenv("TWITTER_ACCESS_TOKEN_SECRET")
        
        if not all([bearer_token, api_key, api_secret, access_token, access_token_secret]):
            raise ValueError("Twitter API anahtarlarÄ± eksik. .env dosyasÄ±nÄ± kontrol edin.")
        
        # Twitter API v2 client
        client = tweepy.Client(
            bearer_token=bearer_token,
            consumer_key=api_key,
            consumer_secret=api_secret,
            access_token=access_token,
            access_token_secret=access_token_secret,
            wait_on_rate_limit=True
        )
        
        return client
    except Exception as e:
        print(f"Twitter API kurulum hatasÄ±: {e}")
        return None

def post_tweet(tweet_text):
    """X platformunda tweet paylaÅŸma"""
    try:
        client = setup_twitter_api()
        if not client:
            return {"success": False, "error": "Twitter API kurulumu baÅŸarÄ±sÄ±z"}
        
        # Tweet uzunluk kontrolÃ¼
        if len(tweet_text) > 280:
            tweet_text = tweet_text[:277] + "..."
        
        response = client.create_tweet(text=tweet_text)
        
        if response.data:
            tweet_id = response.data['id']
            return {
                "success": True, 
                "tweet_id": tweet_id,
                "url": f"https://twitter.com/user/status/{tweet_id}"
            }
        else:
            return {"success": False, "error": "Tweet oluÅŸturulamadÄ±"}
            
    except Exception as e:
        return {"success": False, "error": f"Tweet paylaÅŸÄ±m hatasÄ±: {str(e)}"}

def mark_article_as_posted(article_data, tweet_result):
    """Makaleyi paylaÅŸÄ±ldÄ± olarak iÅŸaretle"""
    try:
        posted_articles = load_json(HISTORY_FILE)
        
        posted_article = {
            "title": article_data.get("title", ""),
            "url": article_data.get("url", ""),
            "hash": article_data.get("hash", ""),
            "posted_date": datetime.now().isoformat(),
            "tweet_id": tweet_result.get("tweet_id", ""),
            "tweet_url": tweet_result.get("url", "")
        }
        
        posted_articles.append(posted_article)
        save_json(HISTORY_FILE, posted_articles)
        
        return True
    except Exception as e:
        print(f"Makale kaydetme hatasÄ±: {e}")
        return False

def check_duplicate_articles():
    """Tekrarlanan makaleleri temizle"""
    try:
        posted_articles = load_json(HISTORY_FILE)
        
        # Son 30 gÃ¼nlÃ¼k makaleleri tut
        cutoff_date = datetime.now() - timedelta(days=30)
        
        filtered_articles = []
        seen_hashes = set()
        
        for article in posted_articles:
            try:
                posted_date = datetime.fromisoformat(article.get("posted_date", ""))
                article_hash = article.get("hash", "")
                
                if posted_date > cutoff_date and article_hash not in seen_hashes:
                    filtered_articles.append(article)
                    seen_hashes.add(article_hash)
            except:
                continue
        
        save_json(HISTORY_FILE, filtered_articles)
        return len(posted_articles) - len(filtered_articles)
        
    except Exception as e:
        print(f"Tekrar temizleme hatasÄ±: {e}")
        return 0

def generate_ai_digest(summaries_with_links, api_key):
    """Eski fonksiyon - geriye dÃ¶nÃ¼k uyumluluk iÃ§in"""
    if not summaries_with_links:
        return "Ã–zet bulunamadÄ±"
    
    # Ä°lk makaleyi kullanarak tweet oluÅŸtur
    first_summary = summaries_with_links[0]
    article_data = {
        "title": "AI Digest",
        "content": first_summary.get("summary", ""),
        "url": first_summary.get("url", "")
    }
    
    return generate_ai_tweet_with_content(article_data, api_key)

def create_pdf(summaries, filename="daily_digest.pdf"):
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    pdf.cell(200, 10, txt="AI Tweet Digest", ln=True, align='C')
    for s in summaries:
        pdf.multi_cell(0, 10, f"â€¢ {s}")
    pdf.output(filename)
    return filename
